pod_name,model_name,batch_size,token_size,GPU_type,iteration,GPU_util,qps,seq_len,FPR,IIPS,MIE
Qwen2.5-14B_batch1_input2048_output10,Qwen2.5-14B,1,2048,L20,10,96.78,1,2048,115.6521,10.948817925581128,5.2770683002232275
Qwen2.5-3B_batch1_input2048_output10,Qwen2.5-3B,1,2048,L20,10,89.74,1,2048,107.2393,43.65545115298872,1.2272185622878895
Qwen3-14B_batch8_input128_output10,Qwen3-14B,8,1024,H800,10,69.95,8,128,86.4756875,46.62793857399754,14.83671637986139
Qwen2.5-14B_batch8_input128_output10,Qwen2.5-14B,8,1024,H800,10,66.27,8,128,81.9262875,43.65817373905925,15.012315996480407
Llama-3.1-8B_batch8_input128_output10,Llama-3.1-8B,8,1024,H800,10,64.0,8,128,79.12,72.45747848972562,8.73560622303125
Qwen2.5-7B_batch8_input128_output10,Qwen2.5-7B,8,1024,H800,10,57.0,8,128,70.46624999999999,68.41324118073088,8.240071516430051
Llama-3.2-3B_batch8_input128_output10,Llama-3.2-3B,8,1024,H800,10,39.55,8,128,48.8936875,84.24211199853255,4.643158756594488
Qwen2.5-3B_batch8_input128_output10,Qwen2.5-3B,8,1024,H800,10,36.26,8,128,44.826425,73.27809376227462,4.893841823497626
Qwen2.5-32B_batch8_input128_output10,Qwen2.5-32B,8,1024,A100,10,92.23,8,128,35.9697,12.544255523270543,22.939392414813927
Qwen3-14B_batch8_input128_output10,Qwen3-14B,8,1024,A800,10,90.43,8,128,35.267700000000005,27.744705007888705,10.169205256274239
Qwen3-14B_batch8_input128_output10,Qwen3-14B,8,1024,A100,10,89.97,8,128,35.0883,25.57536849048366,10.975654176965154
Qwen3-4B_batch8_input128_output10,Qwen3-4B,8,1024,H800,10,28.22,8,128,34.886975,46.880311903230464,5.9533690939621895
Qwen2.5-14B_batch8_input128_output10,Qwen2.5-14B,8,1024,A800,10,88.23,8,128,34.4097,27.535239128127117,9.99728379764842
