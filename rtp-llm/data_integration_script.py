#!/usr/bin/env python3
"""
LLM-Prof æ•°æ®æ•´åˆè„šæœ¬
æ•´åˆ22ä¸ªæ¡ˆä¾‹çš„SEAã€MEAã€OEAä¸‰å±‚æ•°æ®ï¼Œç”¨äºæ¨ªå‘å¯¹æ¯”åˆ†æ
"""

import json
import os
import pandas as pd
import re
from pathlib import Path
from typing import Dict, List, Any, Optional

class LLMProfDataIntegrator:
    def __init__(self, base_dir: str = "."):
        self.base_dir = Path(base_dir)
        self.cases_data = []
        self.hardware_specs = {}
        self.load_hardware_specs()
        
    def load_hardware_specs(self):
        """åŠ è½½ç¡¬ä»¶è§„æ ¼æ•°æ®"""
        hardware_specs_raw = {
            "NVIDIA_A10": {"mem_bandwidth": 600 * (1024**3), "FP16": 125e12, "INT8": 250e12, "memsize": 24 * (1024**3)},
            "NVIDIA_L20": {"mem_bandwidth": 864 * (1024**3), "FP16": 119.5e12, "INT8": 239e12, "memsize": 48 * (1024**3)},
            "NVIDIA_H20_SXM5_96GB": {"mem_bandwidth": 4022 * (1024**3), "FP16": 148e12, "INT8": 296e12, "memsize": 96 * (1024**3)},
            "NVIDIA_H20_SXM5_141GB": {"mem_bandwidth": 4800 * (1024**3), "FP16": 148e12, "INT8": 296e12, "memsize": 141 * (1024**3)},
            "NVIDIA_A100_SXM4_80GB": {"mem_bandwidth": 2039 * (1024**3), "FP16": 312e12, "INT8": 624e12, "memsize": 80 * (1024**3)},
            "NVIDIA_A800_SXM4_80GB": {"mem_bandwidth": 2039 * (1024**3), "FP16": 312e12, "INT8": 624e12, "memsize": 80 * (1024**3)},
            "NVIDIA_H800": {"mem_bandwidth": 3350 * (1024**3), "FP16": 989e12, "INT8": 1979e12, "memsize": 80 * (1024**3)},
            "NVIDIA_H100": {"mem_bandwidth": 3350 * (1024**3), "FP16": 989e12, "INT8": 1979e12, "memsize": 80 * (1024**3)},
            "NVIDIA_H200": {"mem_bandwidth": 4800 * (1024**3), "FP16": 989e12, "INT8": 1979e12, "memsize": 141 * (1024**3)},
            "NVIDIA_B200": {"mem_bandwidth": 8000 * (1024**3), "FP16": 2250e12, "INT8": 4500e12, "memsize": 192 * (1024**3)},
            "AMD_MI308X": {"mem_bandwidth": 4000 * (1024**3), "FP16": 115e12, "INT8": 230e12, "memsize": 192 * (1024**3)}
        }
        
        # è½¬æ¢ä¸ºTFLOPså’ŒGB/så•ä½
        for gpu_name, specs in hardware_specs_raw.items():
            self.hardware_specs[gpu_name] = {
                "mem_bandwidth_gbps": specs["mem_bandwidth"] / (1024**3),
                "fp16_tflops": specs["FP16"] / 1e12,
                "int8_tops": specs["INT8"] / 1e12,
                "memory_size_gb": specs["memsize"] / (1024**3)
            }
    
    def parse_sea_data(self) -> Dict[str, Dict]:
        """è§£æSEAå±‚æ•°æ®"""
        sea_data = {}
        
        # ä»cases_after_sea.txtè§£ææ•°æ®
        cases_file = self.base_dir / "cases_after_sea.txt"
        if not cases_file.exists():
            print(f"Warning: {cases_file} not found")
            return sea_data
            
        with open(cases_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è§£ææ¯ä¸ªåˆ†ç»„çš„æ•°æ®
        groups = {
            "é«˜æ•ˆ": 1,
            "å¤§æ¨¡å‹ä½æ•ˆ": 2, 
            "åˆ©ç”¨ç‡å¤±è¡¡": 3,
            "ç¡¬ä»¶å·®å¼‚": 4
        }
        
        # ä¸­æ–‡ç»„ååˆ°è‹±æ–‡ç»„åçš„æ˜ å°„
        group_name_mapping = {
            "é«˜æ•ˆ": "Low FPR with High QPS or Util",
            "å¤§æ¨¡å‹ä½æ•ˆ": "Large Parameter size with High FPR",
            "åˆ©ç”¨ç‡å¤±è¡¡": "High FPR with Low Util",
            "ç¡¬ä»¶å·®å¼‚": "Same Model with Hardware Diff"
        }
        
        for group_name, group_id in groups.items():
            # æŸ¥æ‰¾åˆ†ç»„æ•°æ®
            pattern = rf"åˆ†ç»„{group_id}.*?:(.*?)(?=åˆ†ç»„|$)"
            match = re.search(pattern, content, re.DOTALL)
            if not match:
                continue
                
            group_content = match.group(1)
            lines = [line.strip() for line in group_content.split('\n') if line.strip()]
            
            for line in lines:
                if line.startswith('åºå·') or not line:
                    continue
                    
                # è§£ææ¯è¡Œæ•°æ®
                parts = line.split()
                if len(parts) >= 10:
                    case_id = parts[0]
                    pod_name = parts[1]
                    model_name = parts[2]
                    gpu_type = parts[3]
                    gpu_num = int(parts[4])
                    qps = float(parts[5])
                    gpu_util = float(parts[6])
                    f_peak = float(parts[7])
                    fpr = float(parts[8])
                    token_size = int(parts[9])
                    
                    # ä½¿ç”¨è‹±æ–‡ç»„å
                    english_group_name = group_name_mapping.get(group_name, group_name)
                    
                    sea_data[case_id] = {
                        "case_id": case_id,
                        "pod_name": pod_name,
                        "model_name": model_name,
                        "gpu_type": gpu_type,
                        "gpu_num": gpu_num,
                        "qps": qps,
                        "gpu_utilization": gpu_util,
                        "f_peak": f_peak,
                        "fpr": fpr,
                        "token_size": token_size,
                        "group_name": english_group_name,
                        "group_id": group_id
                    }
        
        return sea_data
    
    def find_case_directories(self) -> Dict[str, Path]:
        """æŸ¥æ‰¾æ‰€æœ‰æ¡ˆä¾‹ç›®å½•"""
        case_dirs = {}
        
        # æœç´¢traces_after_sea_section_part*ç›®å½•
        for part_dir in self.base_dir.glob("traces_after_sea_section_part*"):
            if part_dir.is_dir():
                for case_dir in part_dir.iterdir():
                    if case_dir.is_dir():
                        case_name = case_dir.name
                        case_dirs[case_name] = case_dir
        
        return case_dirs
    
    def load_mea_data(self, case_dir: Path) -> Optional[Dict]:
        """åŠ è½½MEAæ•°æ®"""
        mea_file = case_dir / "stage4_mea_analysis_results.json"
        if not mea_file.exists():
            return None
            
        try:
            with open(mea_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading MEA data from {mea_file}: {e}")
            return None
    
    def load_oea_data(self, case_dir: Path) -> Optional[Dict]:
        """åŠ è½½OEAæ•°æ®"""
        oea_file = case_dir / "oea_summary.json"
        if not oea_file.exists():
            return None
            
        try:
            with open(oea_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading OEA data from {oea_file}: {e}")
            return None
    
    def load_config_data(self, case_dir: Path) -> Optional[Dict]:
        """åŠ è½½é…ç½®æ•°æ®"""
        config_file = case_dir / "prefill_metrics_with_config.txt"
        if not config_file.exists():
            return None
            
        try:
            config_data = {}
            with open(config_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if ':' in line:
                        key, value = line.split(':', 1)
                        config_data[key.strip()] = value.strip()
            return config_data
        except Exception as e:
            print(f"Error loading config data from {config_file}: {e}")
            return None
    
    def extract_key_metrics(self, case_data: Dict) -> Dict:
        """æå–å…³é”®æŒ‡æ ‡"""
        metrics = {}
        
        # SEAå±‚æŒ‡æ ‡
        sea_data = case_data.get('sea_data', {})
        metrics.update({
            'case_id': sea_data.get('case_id'),
            'pod_name': sea_data.get('pod_name'),
            'model_name': sea_data.get('model_name'),
            'gpu_type': sea_data.get('gpu_type'),
            'gpu_num': sea_data.get('gpu_num'),
            'group_name': sea_data.get('group_name'),
            'group_id': sea_data.get('group_id'),
            'sea_qps': sea_data.get('qps'),
            'sea_fpr': sea_data.get('fpr'),
            'sea_token_size': sea_data.get('token_size'),
        })
        
        # MEAå±‚æŒ‡æ ‡
        mea_data = case_data.get('mea_data', {})
        if mea_data:
            iips_analysis = mea_data.get('mea_analysis', {}).get('iips_analysis', {})
            mie_analysis = mea_data.get('mea_analysis', {}).get('mie_analysis', {})
            
            metrics.update({
                'mea_iips': iips_analysis.get('iips'),
                'mea_total_iterations': iips_analysis.get('total_iterations'),
                'mea_avg_iteration_duration_us': iips_analysis.get('average_iteration_duration_us'),
                'mea_std_iteration_duration_us': iips_analysis.get('std_iteration_duration_us'),
                'mea_mie': mie_analysis.get('mie'),
            })
        
        # ç»Ÿä¸€GPUåˆ©ç”¨ç‡æŒ‡æ ‡ - ä¼˜å…ˆä½¿ç”¨SEAå±‚æ•°æ®ï¼Œå› ä¸ºå®ƒæ˜¯ä¸šåŠ¡å±‚é¢çš„çœŸå®åˆ©ç”¨ç‡
        gpu_utilization = sea_data.get('gpu_utilization')
        if gpu_utilization is None and mea_data:
            # å¦‚æœSEAå±‚æ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨MEAå±‚çš„æ•°æ®ä½œä¸ºå¤‡é€‰
            mie_analysis = mea_data.get('mea_analysis', {}).get('mie_analysis', {})
            gpu_utilization = mie_analysis.get('u_gpu')
        
        metrics['gpu_utilization'] = gpu_utilization
        
        # OEAå±‚æŒ‡æ ‡
        oea_data = case_data.get('oea_data', {})
        if oea_data:
            overall_metrics = oea_data.get('overall_metrics', {})
            bottleneck_ranking = oea_data.get('bottleneck_ranking', [])
            
            metrics.update({
                'oea_overall_efficiency': overall_metrics.get('overall_efficiency'),
                'oea_total_compute_time_us': overall_metrics.get('total_compute_time_us'),
                'oea_total_flops': overall_metrics.get('total_flops'),
                'oea_memory_utilization': overall_metrics.get('overall_memory_utilization'),
            })
            
            # æå–å‰5ä¸ªç“¶é¢ˆç®—å­
            for i, bottleneck in enumerate(bottleneck_ranking[:5]):
                prefix = f'oea_bottleneck_{i+1}'
                metrics.update({
                    f'{prefix}_operator': bottleneck.get('operator_type'),
                    f'{prefix}_score': bottleneck.get('bottleneck_score'),
                    f'{prefix}_efficiency': bottleneck.get('efficiency_degree'),
                    f'{prefix}_time_proportion': bottleneck.get('kernel_time_proportion'),
                })
        
        # ç¡¬ä»¶è§„æ ¼
        gpu_type = metrics.get('gpu_type', '')
        gpu_spec_key = self.map_gpu_type_to_spec_key(gpu_type)
        if gpu_spec_key in self.hardware_specs:
            hw_spec = self.hardware_specs[gpu_spec_key]
            metrics.update({
                'hw_mem_bandwidth_gbps': hw_spec['mem_bandwidth_gbps'],
                'hw_fp16_tflops': hw_spec['fp16_tflops'],
                'hw_memory_size_gb': hw_spec['memory_size_gb'],
            })
        
        return metrics
    
    def map_gpu_type_to_spec_key(self, gpu_type: str) -> str:
        """å°†GPUç±»å‹æ˜ å°„åˆ°ç¡¬ä»¶è§„æ ¼é”®"""
        mapping = {
            'H20': 'NVIDIA_H20_SXM5_96GB',
            'H800': 'NVIDIA_H800',
            'A100': 'NVIDIA_A100_SXM4_80GB',
            'A800': 'NVIDIA_A800_SXM4_80GB',
            'L20': 'NVIDIA_L20'
        }
        return mapping.get(gpu_type, gpu_type)
    
    def integrate_all_data(self) -> List[Dict]:
        """æ•´åˆæ‰€æœ‰æ•°æ®"""
        print("ğŸš€ å¼€å§‹æ•°æ®æ•´åˆ...")
        
        # 1. åŠ è½½SEAæ•°æ®
        print("ğŸ“Š åŠ è½½SEAå±‚æ•°æ®...")
        sea_data = self.parse_sea_data()
        print(f"   æ‰¾åˆ° {len(sea_data)} ä¸ªæ¡ˆä¾‹çš„SEAæ•°æ®")
        
        # 2. æŸ¥æ‰¾æ¡ˆä¾‹ç›®å½•
        print("ğŸ“ æŸ¥æ‰¾æ¡ˆä¾‹ç›®å½•...")
        case_dirs = self.find_case_directories()
        print(f"   æ‰¾åˆ° {len(case_dirs)} ä¸ªæ¡ˆä¾‹ç›®å½•")
        
        # 3. æ•´åˆæ¯ä¸ªæ¡ˆä¾‹çš„æ•°æ®
        integrated_data = []
        
        for case_name, case_dir in case_dirs.items():
            print(f"ğŸ”„ å¤„ç†æ¡ˆä¾‹: {case_name}")
            
            case_data = {
                'case_name': case_name,
                'case_dir': str(case_dir)
            }
            
            # åŒ¹é…SEAæ•°æ®
            matched_sea = None
            for case_id, sea_info in sea_data.items():
                if sea_info['pod_name'] in case_name:
                    matched_sea = sea_info
                    break
            
            if matched_sea:
                case_data['sea_data'] = matched_sea
                print(f"   âœ… SEAæ•°æ®åŒ¹é…æˆåŠŸ (æ¡ˆä¾‹ID: {matched_sea['case_id']})")
            else:
                print(f"   âŒ SEAæ•°æ®åŒ¹é…å¤±è´¥")
                continue
            
            # åŠ è½½MEAæ•°æ®
            mea_data = self.load_mea_data(case_dir)
            if mea_data:
                case_data['mea_data'] = mea_data
                print(f"   âœ… MEAæ•°æ®åŠ è½½æˆåŠŸ")
            else:
                print(f"   âŒ MEAæ•°æ®åŠ è½½å¤±è´¥")
            
            # åŠ è½½OEAæ•°æ®
            oea_data = self.load_oea_data(case_dir)
            if oea_data:
                case_data['oea_data'] = oea_data
                print(f"   âœ… OEAæ•°æ®åŠ è½½æˆåŠŸ")
            else:
                print(f"   âŒ OEAæ•°æ®åŠ è½½å¤±è´¥")
            
            # åŠ è½½é…ç½®æ•°æ®
            config_data = self.load_config_data(case_dir)
            if config_data:
                case_data['config_data'] = config_data
                print(f"   âœ… é…ç½®æ•°æ®åŠ è½½æˆåŠŸ")
            
            integrated_data.append(case_data)
        
        print(f"âœ… æ•°æ®æ•´åˆå®Œæˆï¼Œå…±å¤„ç† {len(integrated_data)} ä¸ªæ¡ˆä¾‹")
        return integrated_data
    
    def export_to_csv(self, integrated_data: List[Dict], output_file: str = "llm_prof_integrated_data.csv"):
        """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
        print(f"ğŸ“¤ å¯¼å‡ºæ•°æ®åˆ° {output_file}...")
        
        # æå–å…³é”®æŒ‡æ ‡
        metrics_list = []
        for case_data in integrated_data:
            metrics = self.extract_key_metrics(case_data)
            metrics_list.append(metrics)
        
        # åˆ›å»ºDataFrameå¹¶å¯¼å‡º
        df = pd.DataFrame(metrics_list)
        df.to_csv(output_file, index=False, encoding='utf-8')
        
        print(f"âœ… æ•°æ®å¯¼å‡ºå®Œæˆ: {output_file}")
        print(f"   å…± {len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—")
        
        return df
    
    def export_to_json(self, integrated_data: List[Dict], output_file: str = "llm_prof_integrated_data.json"):
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        print(f"ğŸ“¤ å¯¼å‡ºå®Œæ•´æ•°æ®åˆ° {output_file}...")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(integrated_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å®Œæ•´æ•°æ®å¯¼å‡ºå®Œæˆ: {output_file}")
        
        return integrated_data

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ LLM-Prof æ•°æ®æ•´åˆå·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®æ•´åˆå™¨
    integrator = LLMProfDataIntegrator()
    
    # æ•´åˆæ‰€æœ‰æ•°æ®
    integrated_data = integrator.integrate_all_data()
    
    # å¯¼å‡ºæ•°æ®
    print("\nğŸ“Š å¯¼å‡ºæ•°æ®...")
    df = integrator.export_to_csv(integrated_data)
    integrator.export_to_json(integrated_data)
    
    # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
    print("\nğŸ“ˆ æ•°æ®æ¦‚è§ˆ:")
    print(f"   æ€»æ¡ˆä¾‹æ•°: {len(df)}")
    print(f"   æ•°æ®åˆ—æ•°: {len(df.columns)}")
    
    # æŒ‰åˆ†ç»„ç»Ÿè®¡
    if 'group_name' in df.columns:
        group_stats = df['group_name'].value_counts()
        print("\nğŸ“Š åˆ†ç»„ç»Ÿè®¡:")
        for group, count in group_stats.items():
            print(f"   {group}: {count} ä¸ªæ¡ˆä¾‹")
    
    # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡ç»Ÿè®¡
    key_metrics = ['sea_fpr', 'mea_iips', 'mea_mie', 'oea_overall_efficiency']
    print("\nğŸ“Š å…³é”®æŒ‡æ ‡ç»Ÿè®¡:")
    for metric in key_metrics:
        if metric in df.columns:
            values = df[metric].dropna()
            if len(values) > 0:
                print(f"   {metric}: å‡å€¼={values.mean():.4f}, æ ‡å‡†å·®={values.std():.4f}, èŒƒå›´=[{values.min():.4f}, {values.max():.4f}]")
    
    print("\nğŸ‰ æ•°æ®æ•´åˆå®Œæˆï¼")

if __name__ == "__main__":
    main()