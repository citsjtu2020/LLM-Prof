#!/usr/bin/env python3
"""
MEA (Model Efficiency Analyzer) - æ¨¡å‹æ•ˆç‡åˆ†æå™¨
åŸºäºè®ºæ–‡æ¡†æ¶å®ç° IIPS å’Œ MIE è®¡ç®—ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ¨ç†æ•ˆç‡

ä¸»è¦åŠŸèƒ½ï¼š
1. è®¡ç®— IIPS (Inference Iterations Per Second) - æ¯ç§’æ¨ç†è¿­ä»£æ•°
2. è®¡ç®— MIE (Model Inference Efficiency) - æ¨¡å‹æ¨ç†æ•ˆç‡
3. æä¾›æ¨¡å‹çº§åˆ«çš„æ€§èƒ½åˆ†æå’Œç“¶é¢ˆè¯†åˆ«
"""

import json
import sys
import logging
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from collections import Counter
import argparse
import os

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def convert_numpy_types(obj):
    """Convert numpy types to native Python types for JSON serialization"""
    if isinstance(obj, (np.bool_, bool)):
        return bool(obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(v) for v in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(v) for v in obj)
    return obj

class MEAAnalyzer:
    """
    Model Efficiency Analyzer (MEA) - æ¨¡å‹æ•ˆç‡åˆ†æå™¨
    
    åŸºäºè®ºæ–‡ Section 3.3 çš„ MEA è®¾è®¡å®ç°ï¼š
    1. åœ¨çº¿æ¨¡å‹æ—¶é—´çº¿è¿½è¸ª
    2. åŸºäºå†…æ ¸çº§æ¨¡å¼å»ºæ¨¡çš„æ¨ç†è¿­ä»£è¯†åˆ«  
    3. è€ƒè™‘æ¨ç†ååé‡å’Œèµ„æºä½¿ç”¨çš„æ¨¡å‹æ•ˆç‡ä¼°è®¡
    """
    
    def __init__(self, gpu_config_file: Optional[str] = None):
        """åˆå§‹åŒ– MEA åˆ†æå™¨"""
        self.gpu_config = self.load_gpu_config(gpu_config_file)
        
        # ç¡®ä¿å¿…è¦çš„GPUé…ç½®å­—æ®µå­˜åœ¨
        if 'u_GPU' not in self.gpu_config or self.gpu_config['u_GPU'] is None:
            self.gpu_config['u_GPU'] = 0.8  # é»˜è®¤GPUåˆ©ç”¨ç‡
            logger.warning("u_GPU not found in config, using default value: 0.8")
        
        logger.info(f"MEA Analyzer initialized with GPU config: {self.gpu_config}")
    
    def load_gpu_config(self, config_file: Optional[str] = None) -> Dict[str, Any]:
        """åŠ è½½ GPU é…ç½®"""
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                logger.info(f"Loaded GPU config from {config_file}")
                return config
            except Exception as e:
                logger.warning(f"Error loading GPU config: {e}")
        
        # é»˜è®¤é…ç½® - åŸºäºH20 GPU
        default_config = {
            'F_peak': 148.0,  # H20 GPU peak TFLOPs (FP16)
            'N_GPU': 1,
            'u_GPU': 0.58,  
            'memory_bandwidth': 4800.0,  # GB/s
            'gpu_model': 'H20',
            'execute_token_size': None
        }
        
        logger.info("Using default H20 GPU configuration")
        return default_config
    
    def load_stage3_results(self, stage3_results_file: str) -> Dict[str, Any]:
        """åŠ è½½ Stage 3 ç»Ÿè®¡éªŒè¯ç»“æœ"""
        logger.info(f"Loading Stage 3 results from {stage3_results_file}")
        
        with open(stage3_results_file, 'r', encoding='utf-8') as f:
            stage3_data = json.load(f)
        
        if stage3_data.get('stage') != 3:
            raise ValueError(f"Invalid Stage 3 results file: expected stage=3, got {stage3_data.get('stage')}")
        
        logger.info(f"Loaded {len(stage3_data.get('iterations', []))} iterations from Stage 3")
        return stage3_data
    
    def load_external_config(self, config_file: str) -> Dict[str, Any]:
        """
        åŠ è½½å¤–éƒ¨é…ç½®æ–‡æ¡£ï¼Œæ”¯æŒprefill_metrics_with_config.txtæ ¼å¼
        è¯¥æ–‡ä»¶åŒ…å«æ¨¡å‹æ¶æ„ä¿¡æ¯ã€ç¡¬ä»¶ç¯å¢ƒä¿¡æ¯å’ŒæœåŠ¡è¿è¡Œæ—¶æŒ‡æ ‡
        
        Args:
            config_file: å¤–éƒ¨é…ç½®æ–‡ä»¶è·¯å¾„ (prefill_metrics_with_config.txt)
            
        Returns:
            åŒ…å«GPUé…ç½®ä¿¡æ¯çš„å­—å…¸
        """
        logger.info(f"Loading external GPU config from {config_file}")
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£æé…ç½®æ–‡ä»¶å†…å®¹
            external_config = self._parse_prefill_metrics_config(content)
            
            # æ›´æ–°GPUé…ç½® - ä»ç¡¬ä»¶ç¯å¢ƒä¿¡æ¯ä¸­æå–
            hardware_info = external_config.get('hardware_info', {})
            if 'f_peak' in hardware_info:
                self.gpu_config['F_peak'] = hardware_info['f_peak']
                logger.info(f"Successfully loaded F_peak from hardware_info: {hardware_info['f_peak']}")
            if 'n_gpu' in hardware_info:
                self.gpu_config['N_GPU'] = hardware_info['n_gpu']
                logger.info(f"Successfully loaded N_GPU from hardware_info: {hardware_info['n_gpu']}")
            if 'memory_bandwidth' in hardware_info:
                self.gpu_config['memory_bandwidth'] = hardware_info['memory_bandwidth']
                logger.info(f"Successfully loaded memory_bandwidth from hardware_info: {hardware_info['memory_bandwidth']}")
            if 'gpu_type' in hardware_info:
                self.gpu_config['gpu_model'] = hardware_info['gpu_type']
                logger.info(f"Successfully loaded gpu_model from hardware_info: {hardware_info['gpu_type']}")
            
            # å¤„ç†NVIDIA GPUè§„æ ¼ - ä»è¯¦ç»†è§„æ ¼ä¸­æå–ä¿¡æ¯
            for gpu_model, specs in hardware_info.items():
                if isinstance(specs, dict) and 'FP16' in specs:
                    extracted_f_peak = specs['FP16'] / 1e12  # è½¬æ¢ä¸ºTFLOPs
                    extracted_bandwidth = specs['mem_bandwidth'] / (1024**3)  # è½¬æ¢ä¸ºGB/s
                    extracted_gpu_type = gpu_model  # ä¿æŒå®Œæ•´çš„GPUå‹å·åç§°
                    
                    # æ›´æ–°GPUé…ç½®ä¿¡æ¯
                    self.gpu_config['F_peak'] = extracted_f_peak
                    self.gpu_config['memory_bandwidth'] = extracted_bandwidth
                    self.gpu_config['gpu_model'] = extracted_gpu_type
                    logger.info(f"Extracted F_peak from {gpu_model} specs: {extracted_f_peak}")
                    logger.info(f"Extracted memory_bandwidth from {gpu_model} specs: {extracted_bandwidth}")
                    logger.info(f"Extracted gpu_model from {gpu_model} specs: {extracted_gpu_type}")
            
            # å¤„ç† GPUè§„æ ¼ - ä»è¯¦ç»†è§„æ ¼ä¸­æå–ä¿¡æ¯
            for gpu_model, specs in hardware_info.items():
                if isinstance(specs, dict) and 'FP16' in specs:
                    self.gpu_config['F_peak'] = specs['FP16'] / 1e12  # è½¬æ¢ä¸ºTFLOPs
                    self.gpu_config['memory_bandwidth'] = specs['mem_bandwidth'] / (1024**3)  # è½¬æ¢ä¸ºGB/s
                    self.gpu_config['gpu_model'] = gpu_model  # ä¿æŒå®Œæ•´çš„GPUå‹å·åç§°ï¼Œä¸è¦æˆªå–
            
            # æ›´æ–°GPUé…ç½® - ä»æœåŠ¡è¿è¡Œæ—¶æŒ‡æ ‡ä¸­æå–
            runtime_metrics = external_config.get('runtime_metrics', {})
            if 'u_gpu' in runtime_metrics:
                self.gpu_config['u_GPU'] = runtime_metrics['u_gpu']  # æ­£ç¡®æ˜ å°„u_gpu -> u_GPU
                logger.info(f"Successfully loaded GPU utilization: {runtime_metrics['u_gpu']}")
            if 'n_gpu' in runtime_metrics:
                # ä¼˜å…ˆä½¿ç”¨runtime_metricsä¸­çš„n_gpuï¼Œå› ä¸ºå®ƒæ˜¯å®é™…è¿è¡Œæ—¶çš„GPUæ•°é‡
                self.gpu_config['N_GPU'] = runtime_metrics['n_gpu']
                logger.info(f"Successfully loaded GPU count: {runtime_metrics['n_gpu']}")
            if 'execute_token_size' in runtime_metrics:
                # è®¡ç®—å¹³å‡token size
                token_data = runtime_metrics['execute_token_size']
                if isinstance(token_data, dict) and 'values' in token_data:
                    avg_token_size = sum(token_data['values']) / len(token_data['values'])
                    self.gpu_config['execute_token_size'] = avg_token_size
                elif isinstance(token_data, (int, float)):
                    self.gpu_config['execute_token_size'] = token_data
            
            # æ·»åŠ æ¨¡å‹ä¿¡æ¯
            model_info = external_config.get('model_info', {})
            if model_info:
                self.gpu_config['model_name'] = model_info.get('model_name', 'Unknown')
                self.gpu_config['app_name'] = model_info.get('app_name', 'Unknown')
                self.gpu_config['inference_engine'] = runtime_metrics.get('inference_engine', 'Unknown')
            
            logger.info(f"Updated GPU config from prefill_metrics_with_config.txt: {self.gpu_config}")
            return external_config
            
        except FileNotFoundError:
            logger.warning(f"External config file {config_file} not found, using default values")
            return {}
        except Exception as e:
            logger.error(f"Error loading external config: {e}")
            return {}
    
    def _parse_prefill_metrics_config(self, content: str) -> Dict[str, Any]:
        """
        è§£æprefill_metrics_with_config.txtæ–‡ä»¶å†…å®¹
        æ”¯æŒæ··åˆæ ¼å¼ï¼šé”®å€¼å¯¹ + JSONå—
        """
        config = {
            'model_info': {},
            'hardware_info': {},
            'runtime_metrics': {}
        }
        
        lines = content.split('\n')
        current_section = None
        in_json_block = False
        json_buffer = ""
        
        for line in lines:
            line = line.strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
            if not line or line.startswith('#'):
                continue
            
            # æ£€æµ‹ç« èŠ‚æ ‡é¢˜
            if 'æ¨¡å‹æ¶æ„ä¿¡æ¯' in line or 'Model Architecture' in line or '1.' in line:
                current_section = 'model_info'
                continue
            elif 'ç¡¬ä»¶ç¯å¢ƒä¿¡æ¯' in line or 'Hardware Environment' in line or '2.' in line:
                current_section = 'hardware_info'
                continue
            elif 'æœåŠ¡è¿è¡Œæ—¶æŒ‡æ ‡' in line or 'Runtime Metrics' in line or '3.' in line:
                current_section = 'runtime_metrics'
                continue
            
            # è§£æé”®å€¼å¯¹æ ¼å¼ (key = value)
            if '=' in line and not in_json_block:
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip()
                
                if current_section == 'model_info':
                    config['model_info'][key] = value
                elif current_section == 'runtime_metrics':
                    # ç‰¹æ®Šå¤„ç†æ•°å€¼ç±»å‹
                    if key == 'u_gpu':
                        try:
                            config['runtime_metrics'][key] = float(value)
                        except ValueError:
                            config['runtime_metrics'][key] = value
                    elif key == 'n_gpu':
                        try:
                            config['runtime_metrics'][key] = int(value)
                        except ValueError:
                            config['runtime_metrics'][key] = value
                    else:
                        config['runtime_metrics'][key] = value
                continue
            
            # æ£€æµ‹JSONå—å¼€å§‹
            if line.startswith('```json'):
                in_json_block = True
                json_buffer = ""
                continue
            elif line.startswith('```') and in_json_block:
                in_json_block = False
                try:
                    # é¢„å¤„ç†JSONå†…å®¹ï¼Œå¤„ç†ç‰¹æ®Šçš„æ—¶é—´åºåˆ—æ•°æ®æ ¼å¼
                    processed_json = self._preprocess_json_content(json_buffer, current_section)
                    json_data = json.loads(processed_json)
                    
                    if current_section == 'hardware_info':
                        # è§£æç¡¬ä»¶ä¿¡æ¯
                        if 'gpu_type' in json_data:
                            config['hardware_info']['gpu_type'] = json_data['gpu_type']
                        if 'f_peak' in json_data:
                            config['hardware_info']['f_peak'] = json_data['f_peak']
                        if 'n_gpu' in json_data:
                            config['hardware_info']['n_gpu'] = json_data['n_gpu']
                        if 'memory_bandwidth' in json_data:
                            config['hardware_info']['memory_bandwidth'] = json_data['memory_bandwidth']
                        # å¤„ç†NVIDIA GPUè§„æ ¼
                        for gpu_model, specs in json_data.items():
                            if isinstance(specs, dict) and 'FP16' in specs:
                                config['hardware_info']['f_peak'] = specs['FP16'] / 1e12  # è½¬æ¢ä¸ºTFLOPs
                                config['hardware_info']['memory_bandwidth'] = specs['mem_bandwidth'] / (1024**3)  # è½¬æ¢ä¸ºGB/s
                                config['hardware_info']['gpu_type'] = gpu_model.split('_')[1] if '_' in gpu_model else gpu_model
                    elif current_section == 'runtime_metrics':
                        # è§£æè¿è¡Œæ—¶æŒ‡æ ‡
                        if 'u_gpu' in json_data:
                            config['runtime_metrics']['u_gpu'] = json_data['u_gpu']
                            logger.info(f"Successfully parsed u_gpu from JSON: {json_data['u_gpu']}")
                        if 'n_gpu' in json_data:
                            config['runtime_metrics']['n_gpu'] = json_data['n_gpu']
                            logger.info(f"Successfully parsed n_gpu from JSON: {json_data['n_gpu']}")
                        if 'qps' in json_data:
                            config['runtime_metrics']['qps'] = json_data['qps']
                        if 'execute_token_size' in json_data:
                            config['runtime_metrics']['execute_token_size'] = json_data['execute_token_size']
                        if 'inference_engine' in json_data:
                            config['runtime_metrics']['inference_engine'] = json_data['inference_engine']
                        if 'task_function' in json_data:
                            config['runtime_metrics']['task_function'] = json_data['task_function']
                except json.JSONDecodeError as e:
                    logger.warning(f"Failed to parse JSON block in section {current_section}: {e}")
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨è§£æå…³é”®å­—æ®µ
                    if current_section == 'runtime_metrics':
                        self._manual_extract_runtime_metrics(json_buffer, config)
                    elif current_section == 'hardware_info':
                        self._manual_extract_hardware_info(json_buffer, config)
                json_buffer = ""
            elif in_json_block:
                json_buffer += line + "\n"
        
        return config
    
    def _preprocess_json_content(self, json_content: str, section: str) -> str:
        """é¢„å¤„ç†JSONå†…å®¹ï¼Œå¤„ç†ç‰¹æ®Šæ ¼å¼çš„æ—¶é—´åºåˆ—æ•°æ®"""
        if section != 'runtime_metrics':
            return json_content
        
        # å¤„ç†æ—¶é—´åºåˆ—æ•°æ®æ ¼å¼ï¼Œå°†å…¶è½¬æ¢ä¸ºæ ‡å‡†JSON
        lines = json_content.split('\n')
        processed_lines = []
        in_time_series = False
        time_series_key = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æµ‹æ—¶é—´åºåˆ—æ•°æ®å¼€å§‹
            if ('"qps":' in line or '"execute_token_size":' in line) and line.endswith(':'):
                time_series_key = line.split(':')[0].strip().strip('"')
                in_time_series = True
                processed_lines.append(f'"{time_series_key}": "time_series_data",')
                continue
            elif in_time_series:
                # è·³è¿‡æ—¶é—´åºåˆ—æ•°æ®è¡Œï¼Œç›´åˆ°é‡åˆ°ä¸‹ä¸€ä¸ªå­—æ®µæˆ–ç»“æŸ
                if line.startswith('"') and ':' in line and not line.startswith('2025-'):
                    in_time_series = False
                    processed_lines.append(line)
                elif line == '}' or line == '},':
                    in_time_series = False
                    processed_lines.append(line)
                # è·³è¿‡æ—¶é—´åºåˆ—æ•°æ®è¡Œ
                continue
            else:
                processed_lines.append(line)
        
        return '\n'.join(processed_lines)
    
    def _manual_extract_runtime_metrics(self, json_buffer: str, config: Dict[str, Any]):
        """æ‰‹åŠ¨æå–è¿è¡Œæ—¶æŒ‡æ ‡ï¼Œå½“JSONè§£æå¤±è´¥æ—¶ä½¿ç”¨"""
        import re
        
        # æå–u_gpu
        u_gpu_match = re.search(r'"u_gpu":\s*([0-9.]+)', json_buffer)
        if u_gpu_match:
            try:
                config['runtime_metrics']['u_gpu'] = float(u_gpu_match.group(1))
                logger.info(f"Manually extracted u_gpu: {config['runtime_metrics']['u_gpu']}")
            except ValueError:
                pass
        
        # æå–n_gpu
        n_gpu_match = re.search(r'"n_gpu":\s*([0-9]+)', json_buffer)
        if n_gpu_match:
            try:
                config['runtime_metrics']['n_gpu'] = int(n_gpu_match.group(1))
                logger.info(f"Manually extracted n_gpu: {config['runtime_metrics']['n_gpu']}")
            except ValueError:
                pass
        
        # æå–inference_engine
        engine_match = re.search(r'"inference_engine":\s*"([^"]+)"', json_buffer)
        if engine_match:
            config['runtime_metrics']['inference_engine'] = engine_match.group(1)
            logger.info(f"Manually extracted inference_engine: {config['runtime_metrics']['inference_engine']}")
        
        # æå–task_function
        task_match = re.search(r'"task_function":\s*"([^"]+)"', json_buffer)
        if task_match:
            config['runtime_metrics']['task_function'] = task_match.group(1)
            logger.info(f"Manually extracted task_function: {config['runtime_metrics']['task_function']}")
    
    def _manual_extract_hardware_info(self, json_buffer: str, config: Dict[str, Any]):
        """æ‰‹åŠ¨æå–ç¡¬ä»¶ä¿¡æ¯ï¼Œå½“JSONè§£æå¤±è´¥æ—¶ä½¿ç”¨"""
        import re
        
        # æå–gpu_type
        gpu_type_match = re.search(r'"gpu_type":\s*"([^"]+)"', json_buffer)
        if gpu_type_match:
            config['hardware_info']['gpu_type'] = gpu_type_match.group(1)
            logger.info(f"Manually extracted gpu_type: {config['hardware_info']['gpu_type']}")
        
        # æå–f_peak
        f_peak_match = re.search(r'"f_peak":\s*([0-9.]+)', json_buffer)
        if f_peak_match:
            try:
                config['hardware_info']['f_peak'] = float(f_peak_match.group(1))
                logger.info(f"Manually extracted f_peak: {config['hardware_info']['f_peak']}")
            except ValueError:
                pass
        
        # æå–n_gpu
        n_gpu_match = re.search(r'"n_gpu":\s*([0-9]+)', json_buffer)
        if n_gpu_match:
            try:
                config['hardware_info']['n_gpu'] = int(n_gpu_match.group(1))
                logger.info(f"Manually extracted n_gpu from hardware_info: {config['hardware_info']['n_gpu']}")
            except ValueError:
                pass
        
        # æå–memory_bandwidth
        mem_bw_match = re.search(r'"memory_bandwidth":\s*([0-9.]+)', json_buffer)
        if mem_bw_match:
            try:
                config['hardware_info']['memory_bandwidth'] = float(mem_bw_match.group(1))
                logger.info(f"Manually extracted memory_bandwidth: {config['hardware_info']['memory_bandwidth']}")
            except ValueError:
                pass
    
    def extract_valid_iterations(self, stage3_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æå–æœ€ç»ˆéªŒè¯é€šè¿‡çš„æœ‰æ•ˆè¿­ä»£"""
        iterations = stage3_data.get('iterations', [])
        valid_iterations = [
            iteration for iteration in iterations 
            if iteration.get('final_validated', False)
        ]
        
        logger.info(f"Extracted {len(valid_iterations)} valid iterations out of {len(iterations)} total")
        return valid_iterations
    
    def calculate_iips(self, valid_iterations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è®¡ç®— IIPS (Inference Iterations Per Second) - åŸºäºç«¯åˆ°ç«¯æ—¶é—´
        
        ä»ç¬¬ä¸€ä¸ªiterationçš„å¼€å§‹æ—¶é—´åˆ°æœ€åä¸€ä¸ªiterationçš„ç»“æŸæ—¶é—´
        
        Args:
            valid_iterations: éªŒè¯é€šè¿‡çš„è¿­ä»£åˆ—è¡¨
            
        Returns:
            åŒ…å« IIPS è®¡ç®—ç»“æœçš„å­—å…¸
        """
        if not valid_iterations:
            logger.warning("No valid iterations found for IIPS calculation")
            return {
                'iips': 0.0,
                'total_iterations': 0,
                'total_duration_us': 0.0,
                'total_duration_s': 0.0,
                'average_iteration_duration_us': 0.0,
                'error': 'No valid iterations'
            }
        
        # è·å–ç¬¬ä¸€ä¸ªiterationçš„å¼€å§‹æ—¶é—´å’Œæœ€åä¸€ä¸ªiterationçš„ç»“æŸæ—¶é—´
        first_start_ts = None
        last_end_ts = None
        iteration_durations = []
        
        for iteration in valid_iterations:
            start_ts = iteration.get('start_ts')
            end_ts = iteration.get('end_ts')
            duration_us = iteration.get('duration_us', 0.0)
            
            if start_ts is not None and end_ts is not None:
                if first_start_ts is None or start_ts < first_start_ts:
                    first_start_ts = start_ts
                if last_end_ts is None or end_ts > last_end_ts:
                    last_end_ts = end_ts
                    
            if duration_us > 0:
                iteration_durations.append(duration_us)
        
        if first_start_ts is None or last_end_ts is None:
            logger.warning("Could not find valid start/end timestamps")
            return {
                'iips': 0.0,
                'total_iterations': len(valid_iterations),
                'total_duration_us': 0.0,
                'total_duration_s': 0.0,
                'average_iteration_duration_us': 0.0,
                'error': 'Invalid timestamp data'
            }
        
        # è®¡ç®—ç«¯åˆ°ç«¯æ€»æ—¶é—´
        total_duration_us = last_end_ts - first_start_ts
        if total_duration_us <= 0:
            logger.warning("Total duration is zero or negative")
            return {
                'iips': 0.0,
                'total_iterations': len(valid_iterations),
                'total_duration_us': total_duration_us,
                'total_duration_s': 0.0,
                'average_iteration_duration_us': 0.0,
                'error': 'Invalid duration data'
            }
        
        # è½¬æ¢ä¸ºç§’
        total_duration_s = total_duration_us / 1_000_000.0
        
        # è®¡ç®— IIPS
        num_iterations = len(valid_iterations)
        iips = num_iterations / total_duration_s
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_duration_us = np.mean(iteration_durations) if iteration_durations else 0.0
        std_duration_us = np.std(iteration_durations) if len(iteration_durations) > 1 else 0.0
        min_duration_us = np.min(iteration_durations) if iteration_durations else 0.0
        max_duration_us = np.max(iteration_durations) if iteration_durations else 0.0
        
        result = {
            'iips': float(iips),
            'total_iterations': num_iterations,
            'total_duration_us': float(total_duration_us),
            'total_duration_s': float(total_duration_s),
            'average_iteration_duration_us': float(avg_duration_us),
            'std_iteration_duration_us': float(std_duration_us),
            'min_iteration_duration_us': float(min_duration_us),
            'max_iteration_duration_us': float(max_duration_us),
            'first_start_ts': float(first_start_ts),
            'last_end_ts': float(last_end_ts),
            'throughput_tokens_per_second': float(iips)  # æ¯ç§’ç”Ÿæˆçš„ token æ•°
        }
        
        logger.info(f"IIPS calculated: {iips:.2f} iterations/second")
        logger.info(f"End-to-end duration: {total_duration_s:.3f} seconds ({total_duration_us:.1f} Î¼s)")
        logger.info(f"Average iteration duration: {avg_duration_us:.2f} Î¼s")
        
        return result
    
    def validate_gpu_config(self) -> bool:
        """
        éªŒè¯GPUé…ç½®æ˜¯å¦å®Œæ•´
        
        Returns:
            é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        required_fields = ['u_GPU', 'F_peak', 'N_GPU']
        missing_fields = []
        
        for field in required_fields:
            if self.gpu_config.get(field) is None:
                missing_fields.append(field)
        
        if missing_fields:
            logger.error(f"Missing required GPU config fields: {missing_fields}")
            logger.error("Please provide external config file with gpu_utilization, gpu_count, and peak_flops")
            return False
        
        return True
    
    def calculate_mie(self, iips_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        è®¡ç®— MIE (Model Inference Efficiency)
        
        æ ¹æ®è®ºæ–‡å…¬å¼ï¼šMIE = (F_peak Ã— u_GPU Ã— N_GPU) / IIPS
        
        Args:
            iips_result: IIPS è®¡ç®—ç»“æœ
            
        Returns:
            åŒ…å« MIE è®¡ç®—ç»“æœçš„å­—å…¸
        """
        # éªŒè¯GPUé…ç½®
        if not self.validate_gpu_config():
            return {
                'mie': float('inf'),
                'error': 'Invalid or incomplete GPU configuration'
            }
        
        iips = iips_result.get('iips', 0.0)
        
        if iips <= 0:
            logger.warning("IIPS is zero or negative, cannot calculate MIE")
            return {
                'mie': float('inf'),
                'f_peak': self.gpu_config['F_peak'],
                'u_gpu': self.gpu_config['u_GPU'],
                'n_gpu': self.gpu_config['N_GPU'],
                'iips': iips,
                'error': 'Invalid IIPS value'
            }
        
        # è®¡ç®— MIE - ä½¿ç”¨å¤–éƒ¨é…ç½®æä¾›çš„çœŸå®GPUåˆ©ç”¨ç‡
        f_peak = self.gpu_config['F_peak']  # TFLOPs
        u_gpu = self.gpu_config['u_GPU']    # ä»å¤–éƒ¨é…ç½®è·å–çš„çœŸå®åˆ©ç”¨ç‡
        n_gpu = self.gpu_config['N_GPU']    # ä»å¤–éƒ¨é…ç½®è·å–çš„GPUæ•°é‡
        
        mie = (f_peak * u_gpu * n_gpu) / iips
        
        # è®¡ç®—ç›¸å…³æŒ‡æ ‡
        effective_compute_power = f_peak * u_gpu * n_gpu  # æœ‰æ•ˆè®¡ç®—èƒ½åŠ› (TFLOPs)
        compute_per_token = mie  # æ¯ä¸ª token æ¶ˆè€—çš„è®¡ç®—èµ„æº (TFLOPs)
        
        result = {
            'mie': float(mie),
            'f_peak': float(f_peak),
            'u_gpu': float(u_gpu),
            'n_gpu': int(n_gpu),
            'iips': float(iips),
            'effective_compute_power_tflops': float(effective_compute_power),
            'compute_per_token_tflops': float(compute_per_token),
            'gpu_model': self.gpu_config.get('gpu_model', 'Unknown'),
            'execute_token_size': self.gpu_config.get('execute_token_size'),
            'efficiency_interpretation': self._interpret_mie(mie),
            'config_source': 'external_config'  # æ ‡æ˜é…ç½®æ¥æº
        }
        
        logger.info(f"MIE calculated: {mie:.6f} TFLOPs per iteration")
        logger.info(f"Using external GPU utilization: {u_gpu:.3f}")
        logger.info(f"Effective compute power: {effective_compute_power:.2f} TFLOPs")
        
        return result
    
    def _interpret_mie(self, mie: float) -> str:
        """è§£é‡Š MIE å€¼çš„å«ä¹‰"""
        if mie < 0.001:
            return "Excellent efficiency - very low compute cost per token"
        elif mie < 0.01:
            return "Good efficiency - reasonable compute cost per token"
        elif mie < 0.1:
            return "Moderate efficiency - moderate compute cost per token"
        elif mie < 1.0:
            return "Poor efficiency - high compute cost per token"
        else:
            return "Very poor efficiency - extremely high compute cost per token"
    
    def analyze_iteration_patterns(self, valid_iterations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè¿­ä»£æ‰§è¡Œæ¨¡å¼"""
        if not valid_iterations:
            return {'error': 'No valid iterations to analyze'}
        
        # åˆ†ææ“ä½œç¬¦åˆ†å¸ƒå˜åŒ–
        operator_distributions = []
        for iteration in valid_iterations:
            op_dist = iteration.get('operator_distribution', {})
            if op_dist:
                operator_distributions.append(op_dist)
        
        if not operator_distributions:
            return {'error': 'No operator distribution data found'}
        
        # è®¡ç®—æ“ä½œç¬¦åˆ†å¸ƒçš„ç¨³å®šæ€§
        all_operators = set()
        for dist in operator_distributions:
            all_operators.update(dist.keys())
        
        operator_stability = {}
        for op in all_operators:
            values = [dist.get(op, 0.0) for dist in operator_distributions]
            operator_stability[op] = {
                'mean': float(np.mean(values)),
                'std': float(np.std(values)),
                'cv': float(np.std(values) / np.mean(values)) if np.mean(values) > 0 else 0.0
            }
        
        # åˆ†ææ‰§è¡Œæ—¶é—´å˜åŒ–
        durations = [it.get('duration_us', 0) for it in valid_iterations if it.get('duration_us', 0) > 0]
        duration_stats = {
            'mean_us': float(np.mean(durations)) if durations else 0.0,
            'std_us': float(np.std(durations)) if durations else 0.0,
            'cv': float(np.std(durations) / np.mean(durations)) if durations and np.mean(durations) > 0 else 0.0,
            'min_us': float(np.min(durations)) if durations else 0.0,
            'max_us': float(np.max(durations)) if durations else 0.0
        }
        
        return {
            'operator_stability': operator_stability,
            'duration_statistics': duration_stats,
            'execution_consistency': 'High' if duration_stats['cv'] < 0.1 else 'Medium' if duration_stats['cv'] < 0.3 else 'Low'
        }
    
    def analyze_model_efficiency(self, stage3_results_file: str, 
                                external_config_file: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„ MEA åˆ†æ"""
        logger.info("Starting MEA (Model Efficiency Analyzer) analysis")
        
        try:
            # åŠ è½½å¤–éƒ¨é…ç½®
            external_config = {}
            if external_config_file:
                external_config = self.load_external_config(external_config_file)
            
            # åŠ è½½ Stage 3 ç»“æœ
            stage3_data = self.load_stage3_results(stage3_results_file)
            
            # æå–æœ‰æ•ˆè¿­ä»£
            valid_iterations = self.extract_valid_iterations(stage3_data)
            
            # è®¡ç®—IIPSï¼ˆåŸºäºç«¯åˆ°ç«¯æ—¶é—´ï¼‰
            iips_result = self.calculate_iips(valid_iterations)
            
            # è®¡ç®— MIE
            mie_result = self.calculate_mie(iips_result)
            
            # åˆ†æè¿­ä»£æ¨¡å¼
            pattern_analysis = self.analyze_iteration_patterns(valid_iterations)
            
            # ç”Ÿæˆæ€§èƒ½æ´å¯Ÿ
            insights = self._generate_performance_insights(iips_result, mie_result)
            
            # æ„å»ºå®Œæ•´ç»“æœ
            result = {
                'mea_analysis': {
                    'iips_analysis': iips_result,
                    'mie_analysis': mie_result,
                    'pattern_analysis': pattern_analysis,
                    'performance_insights': insights
                },
                'stage3_summary': {
                    'total_iterations': stage3_data.get('metadata', {}).get('total_iterations', 0),
                    'final_valid_iterations': stage3_data.get('metadata', {}).get('final_valid_iterations', 0),
                    'validation_rate': stage3_data.get('metadata', {}).get('validation_rate', 0.0),
                    'global_operator_distribution': stage3_data.get('metadata', {}).get('global_operator_distribution', {})
                },
                'gpu_configuration': self.gpu_config,
                'external_configuration': external_config,
                'analysis_metadata': {
                    'framework_version': 'MEA v1.0',
                    'analysis_timestamp': self._get_timestamp(),
                    'input_file': stage3_results_file,
                    'external_config_file': external_config_file,
                    'methodology': 'Based on LLM-Prof MEA framework with end-to-end IIPS calculation'
                }
            }
            
            logger.info("MEA analysis completed successfully")
            return result
            
        except Exception as e:
            logger.error(f"MEA analysis failed: {e}")
            return {
                'error': str(e),
                'stage': 'MEA Analysis',
                'timestamp': self._get_timestamp()
            }
    
    def _generate_performance_insights(self, iips_result: Dict[str, Any], mie_result: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½æ´å¯Ÿ"""
        insights = []
        
        # IIPSåˆ†æ
        iips = iips_result.get('iips', 0)
        avg_duration = iips_result.get('average_iteration_duration_us', 0) / 1000  # è½¬æ¢ä¸ºms
        
        # MIEåˆ†æ
        mie = mie_result.get('mie', float('inf'))
        
        # GPUåˆ©ç”¨ç‡åˆ†æ - æ·»åŠ å®‰å…¨æ£€æŸ¥
        u_gpu = self.gpu_config.get('u_GPU', 0)
        if u_gpu is None:
            u_gpu = 0
        
        # IIPSåˆ†æ
        if iips > 0:
            insights.append(f"ğŸ“Š IIPS: {iips:.1f} iterations/second")
        
        # MIEåˆ†æ
        if mie < float('inf'):
            if mie > 0.1:
                insights.append(f"âš ï¸  High MIE ({mie:.4f}) indicates poor model efficiency - each token requires significant compute resources")
            elif mie > 0.01:
                insights.append(f"âš ï¸  Moderate MIE ({mie:.4f}) suggests room for optimization")
            else:
                insights.append(f"âœ… Good MIE ({mie:.4f}) indicates efficient compute utilization")
        
        # æ‰§è¡Œæ—¶é—´åˆ†æ
        if avg_duration > 0:
            if avg_duration < 5:
                insights.append(f"âœ… Fast iteration execution ({avg_duration:.1f}ms average)")
            elif avg_duration < 20:
                insights.append(f"âš ï¸  Moderate iteration latency ({avg_duration:.1f}ms average)")
            else:
                insights.append(f"âš ï¸  High iteration latency ({avg_duration:.1f}ms average)")
        
        # GPUåˆ©ç”¨ç‡åˆ†æ - æ·»åŠ æ•°å€¼æ£€æŸ¥
        if isinstance(u_gpu, (int, float)) and u_gpu > 0:
            if u_gpu < 0.3:
                insights.append(f"âš ï¸  Low GPU utilization ({u_gpu:.1%}) suggests underutilized compute resources")
            elif u_gpu < 0.7:
                insights.append(f"âš ï¸  Moderate GPU utilization ({u_gpu:.1%}) - potential for improvement")
            else:
                insights.append(f"âœ… High GPU utilization ({u_gpu:.1%}) indicates good resource usage")
        
        # ååé‡åˆ†æ
        if iips > 0:
            if iips > 100:
                insights.append(f"âœ… High throughput ({iips:.1f} iterations/sec) indicates good performance")
            elif iips > 50:
                insights.append(f"âš ï¸  Moderate throughput ({iips:.1f} iterations/sec)")
            else:
                insights.append(f"âš ï¸  Low throughput ({iips:.1f} iterations/sec) may indicate bottlenecks")
        
        # ä¸€è‡´æ€§åˆ†æ
        std_duration = iips_result.get('std_iteration_duration_us', 0) / 1000
        if std_duration > 0 and avg_duration > 0:
            cv = std_duration / avg_duration
            if cv > 0.5:
                insights.append("âš ï¸  High execution variability may indicate dynamic batching or workload variations")
            elif cv > 0.2:
                insights.append("âš ï¸  Moderate execution variability detected")
            else:
                insights.append("âœ… Consistent execution timing")
        
        return insights
    
    def calculate_wall_clock_iips(self, valid_iterations: List[Dict[str, Any]], stage1_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è®¡ç®—åŸºäºç«¯åˆ°ç«¯æ—¶é—´çš„ IIPS (Wall-clock IIPS)
        
        ä»ç¬¬ä¸€ä¸ªæœ‰æ•ˆiterationçš„ç¬¬ä¸€ä¸ªHtoDåˆ°æœ€åä¸€ä¸ªæœ‰æ•ˆiterationçš„æœ€åä¸€ä¸ªDtoHçš„æ€»æ—¶é—´
        
        Args:
            valid_iterations: éªŒè¯é€šè¿‡çš„è¿­ä»£åˆ—è¡¨
            stage1_data: Stage1ç»“æœæ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„iterationä¿¡æ¯
            
        Returns:
            åŒ…å«ç«¯åˆ°ç«¯ IIPS è®¡ç®—ç»“æœçš„å­—å…¸
        """
        if not valid_iterations:
            logger.warning("No valid iterations found for wall-clock IIPS calculation")
            return {
                'wall_clock_iips': 0.0,
                'wall_clock_duration_us': 0.0,
                'wall_clock_duration_s': 0.0,
                'first_htod_ts': None,
                'last_dtoh_ts': None,
                'error': 'No valid iterations'
            }
        
        # è·å–æ‰€æœ‰stage1 iterations
        all_iterations = stage1_data.get('iterations', [])
        if not all_iterations:
            logger.warning("No iterations found in stage1 data")
            return {
                'wall_clock_iips': 0.0,
                'wall_clock_duration_us': 0.0,
                'wall_clock_duration_s': 0.0,
                'error': 'No stage1 iterations'
            }
        
        # è·å–æœ‰æ•ˆiterationçš„IDåˆ—è¡¨
        valid_iteration_ids = set()
        for iteration in valid_iterations:
            # ä»stage3ç»“æœä¸­è·å–iteration_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç´¢å¼•
            iteration_id = iteration.get('iteration_id')
            if iteration_id is None:
                # å¦‚æœæ²¡æœ‰explicit IDï¼Œå°è¯•ä»å…¶ä»–å­—æ®µæ¨æ–­
                for key in ['id', 'index', 'segment_id']:
                    if key in iteration:
                        iteration_id = iteration[key]
                        break
            if iteration_id is not None:
                valid_iteration_ids.add(iteration_id)
        
        # å¦‚æœæ— æ³•è·å–IDï¼Œä½¿ç”¨å‰Nä¸ªiterationsï¼ˆN=æœ‰æ•ˆiterationæ•°é‡ï¼‰
        if not valid_iteration_ids:
            logger.info("No explicit iteration IDs found, using first N iterations")
            valid_iteration_ids = set(range(1, len(valid_iterations) + 1))
        
        # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„iterations
        valid_stage1_iterations = []
        for iteration in all_iterations:
            iteration_id = iteration.get('iteration_id')
            if iteration_id in valid_iteration_ids:
                valid_stage1_iterations.append(iteration)
        
        if not valid_stage1_iterations:
            logger.warning("No matching iterations found between stage1 and stage3")
            return {
                'wall_clock_iips': 0.0,
                'wall_clock_duration_us': 0.0,
                'wall_clock_duration_s': 0.0,
                'error': 'No matching iterations'
            }
        
        # æŒ‰iteration_idæ’åº
        valid_stage1_iterations.sort(key=lambda x: x.get('iteration_id', 0))
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªiterationçš„ç¬¬ä¸€ä¸ªHtoDäº‹ä»¶
        first_htod_ts = None
        first_iteration = valid_stage1_iterations[0]
        for event in first_iteration.get('events', []):
            if event.get('name') == 'Memcpy HtoD (PINNED -> DEVICE)':
                first_htod_ts = event.get('ts')
                break
        
        # æ‰¾åˆ°æœ€åä¸€ä¸ªiterationçš„æœ€åä¸€ä¸ªDtoHäº‹ä»¶
        last_dtoh_ts = None
        last_iteration = valid_stage1_iterations[-1]
        events = last_iteration.get('events', [])
        # ä»åå¾€å‰æŸ¥æ‰¾DtoHäº‹ä»¶
        for event in reversed(events):
            if event.get('name') == 'Memcpy DtoH (DEVICE -> PINNED)':
                # DtoHäº‹ä»¶çš„ç»“æŸæ—¶é—´ = ts + dur
                event_ts = event.get('ts', 0)
                event_dur = event.get('dur', 0)
                last_dtoh_ts = event_ts + event_dur
                break
        
        if first_htod_ts is None or last_dtoh_ts is None:
            logger.warning(f"Could not find HtoD/DtoH events: first_htod_ts={first_htod_ts}, last_dtoh_ts={last_dtoh_ts}")
            return {
                'wall_clock_iips': 0.0,
                'wall_clock_duration_us': 0.0,
                'wall_clock_duration_s': 0.0,
                'first_htod_ts': first_htod_ts,
                'last_dtoh_ts': last_dtoh_ts,
                'error': 'Missing HtoD/DtoH events'
            }
        
        # è®¡ç®—ç«¯åˆ°ç«¯æ—¶é—´
        wall_clock_duration_us = last_dtoh_ts - first_htod_ts
        wall_clock_duration_s = wall_clock_duration_us / 1_000_000.0
        
        # è®¡ç®—wall-clock IIPS
        num_iterations = len(valid_iterations)
        wall_clock_iips = num_iterations / wall_clock_duration_s if wall_clock_duration_s > 0 else 0.0
        
        result = {
            'wall_clock_iips': float(wall_clock_iips),
            'wall_clock_duration_us': float(wall_clock_duration_us),
            'wall_clock_duration_s': float(wall_clock_duration_s),
            'first_htod_ts': float(first_htod_ts),
            'last_dtoh_ts': float(last_dtoh_ts),
            'total_iterations': num_iterations,
            'first_iteration_id': valid_stage1_iterations[0].get('iteration_id'),
            'last_iteration_id': valid_stage1_iterations[-1].get('iteration_id')
        }
        
        logger.info(f"Wall-clock IIPS calculated: {wall_clock_iips:.2f} iterations/second")
        logger.info(f"Wall-clock duration: {wall_clock_duration_s:.3f} seconds ({wall_clock_duration_us:.1f} Î¼s)")
        logger.info(f"First HtoD at: {first_htod_ts:.3f} Î¼s, Last DtoH at: {last_dtoh_ts:.3f} Î¼s")
        
        return result
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def save_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜ MEA åˆ†æç»“æœ"""
        # è½¬æ¢ numpy ç±»å‹
        results_converted = convert_numpy_types(results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_converted, f, indent=2, ensure_ascii=False)
        
        logger.info(f"MEA analysis results saved to {output_file}")
    
    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å° MEA åˆ†ææ‘˜è¦"""
        if 'error' in results:
            print(f"\nâŒ MEA Analysis Error: {results['error']}")
            return
        
        mea = results.get('mea_analysis', {})
        iips_analysis = mea.get('iips_analysis', {})
        mie = mea.get('mie_analysis', {})
        insights = mea.get('performance_insights', [])
        
        print("\n" + "="*80)
        print("ğŸš€ MEA (Model Efficiency Analyzer) Analysis Results")
        print("="*80)
        
        # IIPS ç»“æœ
        print(f"\nğŸ“Š IIPS (Inference Iterations Per Second) Analysis:")
        print(f"   Throughput: {iips_analysis.get('iips', 0):.2f} iterations/second")
        print(f"   Total valid iterations: {iips_analysis.get('total_iterations', 0)}")
        print(f"   End-to-end execution time: {iips_analysis.get('total_duration_s', 0):.3f} seconds")
        print(f"   Average iteration duration: {iips_analysis.get('average_iteration_duration_us', 0)/1000:.2f} ms")
        
        # æ—¶é—´æˆ³ä¿¡æ¯
        if iips_analysis.get('first_start_ts') and iips_analysis.get('last_end_ts'):
            print(f"   First iteration start: {iips_analysis.get('first_start_ts', 0):.3f} Î¼s")
            print(f"   Last iteration end: {iips_analysis.get('last_end_ts', 0):.3f} Î¼s")
        
        # MIE ç»“æœ
        print(f"\nâš¡ MIE (Model Inference Efficiency) Analysis:")
        print(f"   MIE: {mie.get('mie', 0):.6f} TFLOPs per iteration")
        print(f"   GPU utilization: {mie.get('u_gpu', 0):.1%}")
        print(f"   Effective compute power: {mie.get('effective_compute_power_tflops', 0):.2f} TFLOPs")
        print(f"   Compute per token: {mie.get('compute_per_token_tflops', 0):.6f} TFLOPs")
        print(f"   Efficiency level: {mie.get('efficiency_interpretation', 'Unknown')}")
        
        # GPU é…ç½®
        gpu_config = results.get('gpu_configuration', {})
        print(f"\nğŸ–¥ï¸  GPU Configuration:")
        print(f"   Model: {gpu_config.get('gpu_model', 'Unknown')}")
        print(f"   Count: {gpu_config.get('N_GPU', 'Unknown')}")
        print(f"   Peak Performance: {gpu_config.get('F_peak', 'Unknown')} TFLOPs")
        print(f"   Utilization: {gpu_config.get('u_GPU', 'Unknown'):.1%}")
        if gpu_config.get('execute_token_size'):
            print(f"   Execute Token Size: {gpu_config.get('execute_token_size')}")
        
        # æ€§èƒ½æ´å¯Ÿ
        print(f"\nğŸ’¡ Performance Insights:")
        for insight in insights:
            print(f"   {insight}")
        
        print("\n" + "="*80)

def main():
    parser = argparse.ArgumentParser(description='MEA (Model Efficiency Analyzer)')
    parser.add_argument('stage3_results', type=str, help='Path to Stage 3 results JSON file')
    parser.add_argument('--external-config', type=str, help='Path to external configuration file')
    parser.add_argument('--output', type=str, help='Output file path (optional)')
    args = parser.parse_args()
    
    analyzer = MEAAnalyzer()
    results = analyzer.analyze_model_efficiency(args.stage3_results, args.external_config)
    analyzer.print_summary(results)
    
    # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
    if args.output:
        output_file = args.output
    else:
        # åŸºäºè¾“å…¥æ–‡ä»¶è·¯å¾„ç”Ÿæˆè¾“å‡ºè·¯å¾„
        input_dir = os.path.dirname(args.stage3_results)
        output_file = os.path.join(input_dir, 'stage4_mea_analysis_results.json')
    
    analyzer.save_results(results, output_file)

if __name__ == "__main__":
    main()